# Assignment Module 3: Fuzzing and Coverag

### **Step 0: Pulling in Your Code**

```bash
git remote add upstream https://github.com/NYUAppSecCF/appsec_hw1_module2-YOURPART1REPO
git fetch upstream
git merge upstream/main --allow-unrelated-histories
git push
```

### **Step 1**
This section introduces key techniques for evaluating how well your test suite exercises your program and discovering potential vulnerabilities. You'll use *coverage analysis* and *fuzz testing* to expand the scope of your test suite and identify bugs. Each task builds on the work you've done in Module 1 and 2.

---

### **What Is Coverage?**
Coverage measures how much of your code is executed when running your test suite. It provides insights into which parts of your program are exercised by your tests and highlights areas that remain untested (uncovered).

#### Steps for Measuring Coverage:
1. Build `giftcardreader` with the `--coverage` option using `gcc`.
2. Run your test suite.
3. Use `lcov` to generate a coverage report.
   - Look up the hints for using `lcov` on BrightSpace.
   
#### Assignment Task:
1. Identify two lines of code that are *not covered* by your test suite.
2. Create new test cases (`cov1.gft` and `cov2.gft`) to cover these lines.
3. Add these new test cases to the correct `testcases/xxxx` directory.

---

### **What Is Fuzzing?**
Fuzzing is an automated testing technique where a program is executed with a wide variety of inputs (often randomly generated) to find bugs or vulnerabilities. In this assignment, you'll use AFL++ (a popular fuzzer) to fuzz the `giftcardreader`.

#### Steps for Fuzzing:
1. Install AFL++ and follow the [quick-start instructions](https://github.com/AFLplusplus/AFLplusplus#quick-start-fuzzing-with-afl).
2. Seed the fuzzer with your existing `.gft` test files (from Parts 1 and 2) by placing them in AFL's input directory.
3. Let the fuzzer run for at least two hours.
4. Analyze the outputs:
   - **Queue Directory**: Contains new test cases generated by AFL++.
   - **Crashes Directory**: Contains inputs that cause the program to crash.
   - **Hangs Directory**: Contains inputs that cause the program to hang.

#### Run the Gift Card Reader:
Use the following loop to run the `giftcardreader` on the test cases generated by AFL++:

```bash
for f in output/queue/id*; do ./giftcardreader 1 "$f"; done
```

---

### **Assignment Tasks**
1. **Coverage Improvements**:
   - After fuzzing, generate a new coverage report.
   - Verify that the fuzzer-generated test cases reach more parts of the program.
   
2. **Bug Fixing**:
   - Identify two crashes or hangs caused by fuzzer-generated test cases.
   - Fix the root causes of these issues.
   - Include the corresponding test cases (`fuzzer1.gft` and `fuzzer2.gft`) in your test suite to ensure these bugs don’t reappear.

3. **Writeup**:
   - Write a brief explanation in `part3.txt` describing:
     - The bugs found in this part.
     - The fixes you implemented.

---

### **Hints and Tips**
1. **Understanding Coverage**:
   - Coverage reports highlight untested code. Focus on creating targeted test cases for those areas.
   - Use `lcov` to visually inspect which lines remain uncovered.

2. **Fuzzer Optimization**:
   - Use a strong initial set of test cases to guide the fuzzer toward more valuable inputs.
   - Run the program with ASAN enabled (`make asan`) to detect subtle memory issues in the fuzzer-generated tests.

3. **Bug Identification**:
   - Use AddressSanitizer or debuggers like `gdb`/`lldb` to investigate crashes.
   - Reproducibility matters: If a crash isn’t consistent, refine your testing environment to stabilize it.

4. **Fuzzing Efficiency**:
   - Use at your own risk (advanced Fuzzing)
   - Keep fuzzer input/output directories inside the container (if using Docker) to improve speed.
   - For multi-core systems, run multiple fuzzer instances:
     ```bash
     afl-fuzz -i input -o output -M fuzzer1 ./giftcardreader 1 @@
     afl-fuzz -i input -o output -S fuzzer2 ./giftcardreader 1 @@
     afl-fuzz -i input -o output -S fuzzer3 ./giftcardreader 1 @@
     ```

---

### **Submission Instructions**
1. Commit the following to your repository:
   - Updated code.
   - Your created tests: `cov1.gft` and `cov2.gft`.
   - Fuzzer-generated tests: `fuzzer1.gft` and `fuzzer2.gft`.
   - Coverage report writeup: `part3.txt`.

2. Push your work with the following commands:

```bash
git tag -a -m "Completed hw1 part3." hw1p3handin
git push origin main
git push origin hw1p3handin
```

By following these steps, you’ll gain hands-on experience with coverage analysis, fuzz testing, and debugging—key skills in application security.

## What to Submit

To submit your code, please only submit a file called `git_link.txt` that contains the name of your repository to **Gradescope**.
For example, if your repo is located at 'h<span>ttps:</span>//github.com/NYUAppSecCF/appsec-homework-1-module-1-exampleaccount', you would submit a text file named `git_link.txt` with only line that reads with <ins><b>only</b></ins> the following:

    appsec-homework-1-exampleaccount

Remember that <b>Gradescope is not instant</b>. Especially if we have to look into past GitHub action runs. We have a timeout set for 10 minutes, almost all well running code will complete within 5 minutes. Wait for it to complete or timeout before trying to re-run. 

For ease of grading, we ask that you also submit copies of your writeups as part2.txt and part3.txt directly in Gradescope. Please ensure that these writeups are exact copies of the files from your repository, as we have implemented a check to verify the match. **It cannot be blank, this will cause a hash failure!**. For further details on the writeup requirements, please refer to the grading rubric available in Brightspace under the "Assignment Guideline" section.

Your repository should contain:    

* Module 1
  * Your `.github/workflows/hello.yml`
  * At least one signed commit
  * Your modified C code `module1.c'
  * Your commpiled `module1.nyu`
* Module 2
  * In `testcases/invalid`: `crash1.gft`, `crash2.gft`, and `hang.gft`.
  * A text file named `part2.txt` that contains the bug descriptions
    for each of the three test cases
  * A GitHub Actions YML that runs your tests
  * A commit with the fixed version of the code (if you like, this
    commit can also contain the files mentioned above)
* Module 3
  * In `testcases/valid`: include the files `cov1.gft`, `cov2.gft`, and
    in `testcases/invalid`: include the additional files `fuzzer1.gft`,
    and `fuzzer2.gft`
  * A text file named `part3.txt` that contains your writeup
  * An updated Actions YML that runs the new tests
  * A commit with the fixed version of the code (if you like, this
    commit can also contain the files mentioned above)


## Concluding Remarks

Despite the fixes you've made, there are almost certainly still many
bugs lurking in the program. Although it is possible to get to a secure
program by repeatedly finding and fixing bugs (at least when the program
is this small), it's a lot of work, and just because a fuzzer stops
finding bugs doesn't mean that the program is bug-free!

Realistically, this program is probably not salvageable in its current
state. It would be better in this case to rewrite it from scratch,
either in C using a very defensive programming style, or in a safer
language like Python or Rust. In the "clean" directory, you can find
a cleanly written version of the program (written in C) that
should be relatively bug-free [1]. You'll notice that it's a lot more
verbose, and checks for many more errors than the buggy
version---writing safe C code is difficult!

[1] Although you are encouraged to try to prove us wrong by finding bugs
    in it!
